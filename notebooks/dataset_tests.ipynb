{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, SequentialSampler #random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "import sqlite3\n",
    "import math\n",
    "from torch import default_generator, randperm\n",
    "from torch._utils import _accumulate\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "  (xx, y) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  print(x_lens)\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "  pad_mask = torch.zeros_like(xx_pad[:, :, 0]).type(torch.bool)\n",
    "  for i, length in enumerate(x_lens):\n",
    "    pad_mask[i, length:] = True\n",
    "\n",
    "  return xx_pad, torch.tensor(y), pad_mask\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, \n",
    "               db_path: str, \n",
    "               event_no_list: List[int], #event_no_list_path: str,\n",
    "               pulsemap: str,\n",
    "               input_cols: List[str],\n",
    "               target_cols: List[str],\n",
    "               truth_table: str = \"truth\"\n",
    "               ):\n",
    "    self.db_path = db_path\n",
    "    self.event_no_list = event_no_list #self.event_no_list_path = event_no_list_path\n",
    "    self.pulsemap = pulsemap\n",
    "    self.input_cols = input_cols\n",
    "    self.target_cols = target_cols\n",
    "    self.truth_table = truth_table\n",
    "\n",
    "\n",
    "    if isinstance(list(input_cols), list):\n",
    "      self.input_cols_str = \", \".join(input_cols)\n",
    "    else:\n",
    "\n",
    "      self.input_cols_str = input_cols\n",
    "\n",
    "    if isinstance(target_cols, list):\n",
    "      self.target_cols_str = \", \".join(target_cols)\n",
    "    else:\n",
    "      self.target_cols_str = target_cols\n",
    "    \n",
    "    # self.event_no_list = np.genfromtxt(self.event_no_list_path,dtype=int)\n",
    "\n",
    "    self.data_len = len(self.event_no_list)\n",
    "    \n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    event_no = self.event_no_list[index]\n",
    "    with sqlite3.connect(self.db_path) as conn:\n",
    "      features = Tensor(conn.execute(f\"SELECT {self.input_cols_str} FROM {self.pulsemap} WHERE event_no == {event_no}\").fetchall())\n",
    "      truth = Tensor(conn.execute(f\"SELECT {self.target_cols_str} FROM {self.truth_table} WHERE event_no == {event_no}\").fetchall())\n",
    "    return features, truth\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.data_len\n",
    "\n",
    "class SimpleIceCubeSQLDatamodule(LightningDataModule):\n",
    "    \"\"\"Example of LightningDataModule for MNIST dataset.\n",
    "\n",
    "    A DataModule implements 5 key methods:\n",
    "\n",
    "        def prepare_data(self):\n",
    "            # things to do on 1 GPU/TPU (not on every GPU/TPU in DDP)\n",
    "            # download data, pre-process, split, save to disk, etc...\n",
    "        def setup(self, stage):\n",
    "            # things to do on every process in DDP\n",
    "            # load data, set variables, etc...\n",
    "        def train_dataloader(self):\n",
    "            # return train dataloader\n",
    "        def val_dataloader(self):\n",
    "            # return validation dataloader\n",
    "        def test_dataloader(self):\n",
    "            # return test dataloader\n",
    "        def teardown(self):\n",
    "            # called on every process in DDP\n",
    "            # clean up after fit or test\n",
    "\n",
    "    This allows you to share a full dataset without explaining how to download,\n",
    "    split, transform and process the data.\n",
    "\n",
    "    Read the docs:\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/dev_step4_upgrade_028_with_noise_dynedge_pulsemap_v3_merger_aftercrash.db\",\n",
    "        event_no_list_path: str = \"/groups/icecube/moust/storage/event_selections/event_no_numu_track_energy_15_200_nhits_4_400_sorted.csv\",\n",
    "        pulsemap: str = \"SplitInIcePulses_dynedge_v2_Pulses\",\n",
    "        input_cols: List[str] = [\"charge\", \"dom_time\", \"dom_x\", \"dom_y\", \"dom_z\", \"pmt_dir_x\", \"pmt_dir_y\", \"pmt_dir_z\" ],\n",
    "        target_cols: List[str] = \"energy\",\n",
    "        truth_table: str = \"truth\",\n",
    "        data_dir: str = \"data/\",\n",
    "        # train_val_test_split: Tuple[float, float, float] = (0.8, 0.1, 0.1),# train_val_test_split_rate: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "        batch_size: int = 2,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.db_path = db_path\n",
    "        self.event_no_list_path = event_no_list_path\n",
    "        self.pulsemap = pulsemap\n",
    "        self.input_cols = input_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.truth_table = truth_table\n",
    "        self.data_dir = data_dir\n",
    "        # train_val_test_split: Tuple[float, float, float] = (0.8, 0.1, 0.1),# train_val_test_split_rate: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # also ensures init params will be stored in ckpt\n",
    "        # self.save_hyperparameters(logger=False)\n",
    "\n",
    "        # data transformations here if any\n",
    "        self.event_no_list = pd.read_csv(event_no_list_path,header=0,names=[\"event_no\"],index_col=None)[\"event_no\"].to_numpy()\n",
    "\n",
    "        # self.event_no_list = np.genfromtxt(self.hparams.event_no_list_path,dtype=int)\n",
    "    \n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "    # @property\n",
    "    # def num_classes(self):\n",
    "    #     return 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download data if needed.\n",
    "\n",
    "        Do not use it to assign state (self.x = y).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n",
    "\n",
    "        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n",
    "        careful not to execute things like random split twice!\n",
    "        \"\"\"\n",
    "        # sampler = SequentialSampler()\n",
    "        if not self.data_train and not self.data_val and not self.data_test:\n",
    "            self.data_train= SimpleDataset(\n",
    "                db_path = self.db_path, \n",
    "                event_no_list = self.event_no_list[self.event_no_list % 10 > 1], #event_no_list_path = self.hparams.event_no_list_path,\n",
    "                pulsemap = self.pulsemap,\n",
    "                input_cols = self.input_cols,\n",
    "                target_cols = self.target_cols,\n",
    "                truth_table = self.truth_table,\n",
    "            )\n",
    "            self.data_val= SimpleDataset(\n",
    "                db_path = self.db_path,\n",
    "                event_no_list = self.event_no_list[self.event_no_list % 10 == 1], #event_no_list_path = self.hparams.event_no_list_path,\n",
    "                pulsemap = self.pulsemap,\n",
    "                input_cols = self.input_cols,\n",
    "                target_cols = self.target_cols,\n",
    "                truth_table = self.truth_table,\n",
    "            )\n",
    "            self.data_test= SimpleDataset(\n",
    "                db_path = self.db_path,\n",
    "                event_no_list = self.event_no_list[self.event_no_list% 10 == 0], #event_no_list_path = self.hparams.event_no_list_path,\n",
    "                pulsemap = self.pulsemap,\n",
    "                input_cols = self.input_cols,\n",
    "                target_cols = self.target_cols,\n",
    "                truth_table = self.truth_table,\n",
    "            )\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn= pad_collate,\n",
    "            sampler=SequentialSampler(self.data_train)\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn= pad_collate,\n",
    "            sampler=SequentialSampler(self.data_val)\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn= pad_collate,\n",
    "            sampler=SequentialSampler(self.data_test)\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None):\n",
    "        \"\"\"Clean up after fit or test.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Extra things to save to checkpoint.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]):\n",
    "        \"\"\"Things to do when loading checkpoint.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "dm=SimpleIceCubeSQLDatamodule()\n",
    "dm.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53916"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fbfc15f2940>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = dm.train_dataloader()\n",
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200]\n"
     ]
    }
   ],
   "source": [
    "features, truth, pad_mask = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 200, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 66.5481, 112.6614])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c5fb4c392dc910b689950aeeefba71605df50be4f2015b0a69feb34d143fb9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
