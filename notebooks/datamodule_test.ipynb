{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnMissingException(Exception):\n",
    "    \"\"\"Exception to indicate a missing column in a dataset.\"\"\"\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset, Configurable, LoggerMixin, ABC):\n",
    "    \"\"\"Base Dataset class for reading from any intermediate file format.\"\"\"\n",
    "\n",
    "    # Class method(s)\n",
    "    @classmethod\n",
    "    def from_config(  # type: ignore[override]\n",
    "        cls,\n",
    "        source: Union[DatasetConfig, str],\n",
    "    ) -> Union[\n",
    "        \"Dataset\",\n",
    "        ConcatDataset,\n",
    "        Dict[str, \"Dataset\"],\n",
    "        Dict[str, ConcatDataset],\n",
    "    ]:\n",
    "        \"\"\"Construct `Dataset` instance from `source` configuration.\"\"\"\n",
    "        if isinstance(source, str):\n",
    "            source = DatasetConfig.load(source)\n",
    "\n",
    "        assert isinstance(source, DatasetConfig), (\n",
    "            f\"Argument `source` of type ({type(source)}) is not a \"\n",
    "            \"`DatasetConfig\"\n",
    "        )\n",
    "\n",
    "        # Parse set of `selection``.\n",
    "        if isinstance(source.selection, dict):\n",
    "            return cls._construct_datasets_from_dict(source)\n",
    "        elif (\n",
    "            isinstance(source.selection, list)\n",
    "            and len(source.selection)\n",
    "            and isinstance(source.selection[0], str)\n",
    "        ):\n",
    "            return cls._construct_dataset_from_list_of_strings(source)\n",
    "\n",
    "        return source._dataset_class(**source.dict())\n",
    "\n",
    "    @classmethod\n",
    "    def concatenate(\n",
    "        cls,\n",
    "        datasets: List[\"Dataset\"],\n",
    "    ) -> ConcatDataset:\n",
    "        \"\"\"Concatenate multiple `Dataset`s into one instance.\"\"\"\n",
    "        return ConcatDataset(datasets)\n",
    "\n",
    "    @classmethod\n",
    "    def _construct_datasets_from_dict(\n",
    "        cls, config: DatasetConfig\n",
    "    ) -> Dict[str, \"Dataset\"]:\n",
    "        \"\"\"Construct `Dataset` for each entry in dict `self.selection`.\"\"\"\n",
    "        assert isinstance(config.selection, dict)\n",
    "        datasets: Dict[str, \"Dataset\"] = {}\n",
    "        selections: Dict[str, Union[str, List]] = deepcopy(config.selection)\n",
    "        for key, selection in selections.items():\n",
    "            config.selection = selection\n",
    "            dataset = Dataset.from_config(config)\n",
    "            assert isinstance(dataset, (Dataset, ConcatDataset))\n",
    "            datasets[key] = dataset\n",
    "\n",
    "        # Reset `selections`.\n",
    "        config.selection = selections\n",
    "\n",
    "        return datasets\n",
    "\n",
    "    @classmethod\n",
    "    def _construct_dataset_from_list_of_strings(\n",
    "        cls, config: DatasetConfig\n",
    "    ) -> \"Dataset\":\n",
    "        \"\"\"Construct `Dataset` for each entry in list `self.selection`.\"\"\"\n",
    "        assert isinstance(config.selection, list)\n",
    "        datasets: List[\"Dataset\"] = []\n",
    "        selections: List[str] = deepcopy(cast(List[str], config.selection))\n",
    "        for selection in selections:\n",
    "            config.selection = selection\n",
    "            dataset = Dataset.from_config(config)\n",
    "            assert isinstance(dataset, Dataset)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        # Reset `selections`.\n",
    "        config.selection = selections\n",
    "\n",
    "        return cls.concatenate(datasets)\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_graphnet_paths(\n",
    "        cls, path: Union[str, List[str]]\n",
    "    ) -> Union[str, List[str]]:\n",
    "        if isinstance(path, list):\n",
    "            return [cast(str, cls._resolve_graphnet_paths(p)) for p in path]\n",
    "\n",
    "        assert isinstance(path, str)\n",
    "        return (\n",
    "            path.replace(\"$graphnet\", GRAPHNET_ROOT_DIR)\n",
    "            .replace(\"$GRAPHNET\", GRAPHNET_ROOT_DIR)\n",
    "            .replace(\"${graphnet}\", GRAPHNET_ROOT_DIR)\n",
    "            .replace(\"${GRAPHNET}\", GRAPHNET_ROOT_DIR)\n",
    "        )\n",
    "\n",
    "    @save_dataset_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: Union[str, List[str]],\n",
    "        pulsemaps: Union[str, List[str]],\n",
    "        features: List[str],\n",
    "        truth: List[str],\n",
    "        *,\n",
    "        node_truth: Optional[List[str]] = None,\n",
    "        index_column: str = \"event_no\",\n",
    "        truth_table: str = \"truth\",\n",
    "        node_truth_table: Optional[str] = None,\n",
    "        string_selection: Optional[List[int]] = None,\n",
    "        selection: Optional[Union[str, List[int], List[List[int]]]] = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        loss_weight_table: Optional[str] = None,\n",
    "        loss_weight_column: Optional[str] = None,\n",
    "        loss_weight_default_value: Optional[float] = None,\n",
    "        seed: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Construct Dataset.\n",
    "\n",
    "        Args:\n",
    "            path: Path to the file(s) from which this `Dataset` should read.\n",
    "            pulsemaps: Name(s) of the pulse map series that should be used to\n",
    "                construct the nodes on the individual graph objects, and their\n",
    "                features. Multiple pulse series maps can be used, e.g., when\n",
    "                different DOM types are stored in different maps.\n",
    "            features: List of columns in the input files that should be used as\n",
    "                node features on the graph objects.\n",
    "            truth: List of event-level columns in the input files that should\n",
    "                be used added as attributes on the  graph objects.\n",
    "            node_truth: List of node-level columns in the input files that\n",
    "                should be used added as attributes on the graph objects.\n",
    "            index_column: Name of the column in the input files that contains\n",
    "                unique indicies to identify and map events across tables.\n",
    "            truth_table: Name of the table containing event-level truth\n",
    "                information.\n",
    "            node_truth_table: Name of the table containing node-level truth\n",
    "                information.\n",
    "            string_selection: Subset of strings for which data should be read\n",
    "                and used to construct graph objects. Defaults to None, meaning\n",
    "                all strings for which data exists are used.\n",
    "            selection: The events that should be read. This can be given either\n",
    "                as list of indicies (in `index_column`); or a string-based\n",
    "                selection used to query the `Dataset` for events passing the\n",
    "                selection. Defaults to None, meaning that all events in the\n",
    "                input files are read.\n",
    "            dtype: Type of the feature tensor on the graph objects returned.\n",
    "            loss_weight_table: Name of the table containing per-event loss\n",
    "                weights.\n",
    "            loss_weight_column: Name of the column in `loss_weight_table`\n",
    "                containing per-event loss weights. This is also the name of the\n",
    "                corresponding attribute assigned to the graph object.\n",
    "            loss_weight_default_value: Default per-event loss weight.\n",
    "                NOTE: This default value is only applied when\n",
    "                `loss_weight_table` and `loss_weight_column` are specified, and\n",
    "                in this case to events with no value in the corresponding\n",
    "                table/column. That is, if no per-event loss weight table/column\n",
    "                is provided, this value is ignored. Defaults to None.\n",
    "            seed: Random number generator seed, used for selecting a random\n",
    "                subset of events when resolving a string-based selection (e.g.,\n",
    "                `\"10000 random events ~ event_no % 5 > 0\"` or `\"20% random\n",
    "                events ~ event_no % 5 > 0\"`).\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        if isinstance(pulsemaps, str):\n",
    "            pulsemaps = [pulsemaps]\n",
    "\n",
    "        assert isinstance(features, (list, tuple))\n",
    "        assert isinstance(truth, (list, tuple))\n",
    "\n",
    "        # Resolve reference to `$GRAPHNET` in path(s)\n",
    "        path = self._resolve_graphnet_paths(path)\n",
    "\n",
    "        # Member variable(s)\n",
    "        self._path = path\n",
    "        self._selection = None\n",
    "        self._pulsemaps = pulsemaps\n",
    "        self._features = [index_column] + features\n",
    "        self._truth = [index_column] + truth\n",
    "        self._index_column = index_column\n",
    "        self._truth_table = truth_table\n",
    "        self._loss_weight_default_value = loss_weight_default_value\n",
    "\n",
    "        if node_truth is not None:\n",
    "            assert isinstance(node_truth_table, str)\n",
    "            if isinstance(node_truth, str):\n",
    "                node_truth = [node_truth]\n",
    "\n",
    "        self._node_truth = node_truth\n",
    "        self._node_truth_table = node_truth_table\n",
    "\n",
    "        if string_selection is not None:\n",
    "            self.warning(\n",
    "                (\n",
    "                    \"String selection detected.\\n \"\n",
    "                    f\"Accepted strings: {string_selection}\\n \"\n",
    "                    \"All other strings are ignored!\"\n",
    "                )\n",
    "            )\n",
    "            if isinstance(string_selection, int):\n",
    "                string_selection = [string_selection]\n",
    "\n",
    "        self._string_selection = string_selection\n",
    "\n",
    "        self._selection = None\n",
    "        if self._string_selection:\n",
    "            self._selection = f\"string in {str(tuple(self._string_selection))}\"\n",
    "\n",
    "        self._loss_weight_column = loss_weight_column\n",
    "        self._loss_weight_table = loss_weight_table\n",
    "        if (self._loss_weight_table is None) and (\n",
    "            self._loss_weight_column is not None\n",
    "        ):\n",
    "            self.warning(\"Error: no loss weight table specified\")\n",
    "            assert isinstance(self._loss_weight_table, str)\n",
    "        if (self._loss_weight_table is not None) and (\n",
    "            self._loss_weight_column is None\n",
    "        ):\n",
    "            self.warning(\"Error: no loss weight column specified\")\n",
    "            assert isinstance(self._loss_weight_column, str)\n",
    "\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._label_fns: Dict[str, Callable[[Data], Any]] = {}\n",
    "\n",
    "        self._string_selection_resolver = StringSelectionResolver(\n",
    "            self,\n",
    "            index_column=index_column,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # Implementation-specific initialisation.\n",
    "        self._init()\n",
    "\n",
    "        # Set unique indices\n",
    "        self._indices: Union[List[int], List[List[int]]]\n",
    "        if selection is None:\n",
    "            self._indices = self._get_all_indices()\n",
    "        elif isinstance(selection, str):\n",
    "            self._indices = self._resolve_string_selection_to_indices(\n",
    "                selection\n",
    "            )\n",
    "        else:\n",
    "            self._indices = selection\n",
    "\n",
    "        # Purely internal member variables\n",
    "        self._missing_variables: Dict[str, List[str]] = {}\n",
    "        self._remove_missing_columns()\n",
    "\n",
    "        # Implementation-specific post-init code.\n",
    "        self._post_init()\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__()\n",
    "\n",
    "    # Properties\n",
    "    @property\n",
    "    def path(self) -> Union[str, List[str]]:\n",
    "        \"\"\"Path to the file(s) from which this `Dataset` reads.\"\"\"\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def truth_table(self) -> str:\n",
    "        \"\"\"Name of the table containing event-level truth information.\"\"\"\n",
    "        return self._truth_table\n",
    "\n",
    "    # Abstract method(s)\n",
    "    @abstractmethod\n",
    "    def _init(self) -> None:\n",
    "        \"\"\"Set internal representation needed to read data from input file.\"\"\"\n",
    "\n",
    "    def _post_init(self) -> None:\n",
    "        \"\"\"Implemenation-specific code to be run after the main constructor.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_all_indices(self) -> List[int]:\n",
    "        \"\"\"Return a list of all available values in `self._index_column`.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_event_index(\n",
    "        self, sequential_index: Optional[int]\n",
    "    ) -> Optional[int]:\n",
    "        \"\"\"Return a the event index corresponding to a `sequential_index`.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def query_table(\n",
    "        self,\n",
    "        table: str,\n",
    "        columns: Union[List[str], str],\n",
    "        sequential_index: Optional[int] = None,\n",
    "        selection: Optional[str] = None,\n",
    "    ) -> List[Tuple[Any, ...]]:\n",
    "        \"\"\"Query a table at a specific index, optionally with some selection.\n",
    "\n",
    "        Args:\n",
    "            table: Table to be queried.\n",
    "            columns: Columns to read out.\n",
    "            sequential_index: Sequentially numbered index\n",
    "                (i.e. in [0,len(self))) of the event to query. This _may_\n",
    "                differ from the indexation used in `self._indices`. If no value\n",
    "                is provided, the entire column is returned.\n",
    "            selection: Selection to be imposed before reading out data.\n",
    "                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List of tuples containing the values in `columns`. If the `table`\n",
    "                contains only scalar data for `columns`, a list of length 1 is\n",
    "                returned\n",
    "\n",
    "        Raises:\n",
    "            ColumnMissingException: If one or more element in `columns` is not\n",
    "                present in `table`.\n",
    "        \"\"\"\n",
    "\n",
    "    # Public method(s)\n",
    "    def add_label(self, key: str, fn: Callable[[Data], Any]) -> None:\n",
    "        \"\"\"Add custom graph label define using function `fn`.\"\"\"\n",
    "        assert (\n",
    "            key not in self._label_fns\n",
    "        ), f\"A custom label {key} has already been defined.\"\n",
    "        self._label_fns[key] = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of graphs in `Dataset`.\"\"\"\n",
    "        return len(self._indices)\n",
    "\n",
    "    def __getitem__(self, sequential_index: int) -> Data:\n",
    "        \"\"\"Return graph `Data` object at `index`.\"\"\"\n",
    "        if not (0 <= sequential_index < len(self)):\n",
    "            raise IndexError(\n",
    "                f\"Index {sequential_index} not in range [0, {len(self) - 1}]\"\n",
    "            )\n",
    "        features, truth, node_truth, loss_weight = self._query(\n",
    "            sequential_index\n",
    "        )\n",
    "        graph = self._create_graph(features, truth, node_truth, loss_weight)\n",
    "        return graph\n",
    "\n",
    "    # Internal method(s)\n",
    "    def _resolve_string_selection_to_indices(\n",
    "        self, selection: str\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Resolve selection as string to list of indicies.\n",
    "\n",
    "        Selections are expected to have pandas.DataFrame.query-compatible\n",
    "        syntax, e.g., ``` \"event_no % 5 > 0\" ``` Selections may also specify a\n",
    "        fixed number of events to randomly sample, e.g., ``` \"10000 random\n",
    "        events ~ event_no % 5 > 0\" \"20% random events ~ event_no % 5 > 0\" ```\n",
    "        \"\"\"\n",
    "        return self._string_selection_resolver.resolve(selection)\n",
    "\n",
    "    def _remove_missing_columns(self) -> None:\n",
    "        \"\"\"Remove columns that are not present in the input file.\n",
    "\n",
    "        Columns are removed from `self._features` and `self._truth`.\n",
    "        \"\"\"\n",
    "        # Check if table is completely empty\n",
    "        if len(self) == 0:\n",
    "            self.warning(\"Dataset is empty.\")\n",
    "            return\n",
    "\n",
    "        # Find missing features\n",
    "        missing_features_set = set(self._features)\n",
    "        for pulsemap in self._pulsemaps:\n",
    "            missing = self._check_missing_columns(self._features, pulsemap)\n",
    "            missing_features_set = missing_features_set.intersection(missing)\n",
    "\n",
    "        missing_features = list(missing_features_set)\n",
    "\n",
    "        # Find missing truth variables\n",
    "        missing_truth_variables = self._check_missing_columns(\n",
    "            self._truth, self._truth_table\n",
    "        )\n",
    "\n",
    "        # Remove missing features\n",
    "        if missing_features:\n",
    "            self.warning(\n",
    "                \"Removing the following (missing) features: \"\n",
    "                + \", \".join(missing_features)\n",
    "            )\n",
    "            for missing_feature in missing_features:\n",
    "                self._features.remove(missing_feature)\n",
    "\n",
    "        # Remove missing truth variables\n",
    "        if missing_truth_variables:\n",
    "            self.warning(\n",
    "                (\n",
    "                    \"Removing the following (missing) truth variables: \"\n",
    "                    + \", \".join(missing_truth_variables)\n",
    "                )\n",
    "            )\n",
    "            for missing_truth_variable in missing_truth_variables:\n",
    "                self._truth.remove(missing_truth_variable)\n",
    "\n",
    "    def _check_missing_columns(\n",
    "        self,\n",
    "        columns: List[str],\n",
    "        table: str,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Return a list missing columns in `table`.\"\"\"\n",
    "        for column in columns:\n",
    "            try:\n",
    "                self.query_table(table, [column], 0)\n",
    "            except ColumnMissingException:\n",
    "                if table not in self._missing_variables:\n",
    "                    self._missing_variables[table] = []\n",
    "                self._missing_variables[table].append(column)\n",
    "            except IndexError:\n",
    "                self.warning(f\"Dataset contains no entries for {column}\")\n",
    "\n",
    "        return self._missing_variables.get(table, [])\n",
    "\n",
    "    def _query(\n",
    "        self, sequential_index: int\n",
    "    ) -> Tuple[\n",
    "        List[Tuple[float, ...]],\n",
    "        Tuple[Any, ...],\n",
    "        Optional[List[Tuple[Any, ...]]],\n",
    "        Optional[float],\n",
    "    ]:\n",
    "        \"\"\"Query file for event features and truth information.\n",
    "\n",
    "        The returned lists have lengths correspondings to the number of pulses\n",
    "        in the event. Their constituent tuples have lengths corresponding to\n",
    "        the number of features/attributes in each output\n",
    "\n",
    "        Args:\n",
    "            sequential_index: Sequentially numbered index\n",
    "                (i.e. in [0,len(self))) of the event to query. This _may_\n",
    "                differ from the indexation used in `self._indices`.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing pulse-level event features; event-level truth\n",
    "                information; pulse-level truth information; and event-level\n",
    "                loss weights, respectively.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for pulsemap in self._pulsemaps:\n",
    "            features_pulsemap = self.query_table(\n",
    "                pulsemap, self._features, sequential_index, self._selection\n",
    "            )\n",
    "            features.extend(features_pulsemap)\n",
    "\n",
    "        truth: Tuple[Any, ...] = self.query_table(\n",
    "            self._truth_table, self._truth, sequential_index\n",
    "        )[0]\n",
    "        if self._node_truth:\n",
    "            assert self._node_truth_table is not None\n",
    "            node_truth = self.query_table(\n",
    "                self._node_truth_table,\n",
    "                self._node_truth,\n",
    "                sequential_index,\n",
    "                self._selection,\n",
    "            )\n",
    "        else:\n",
    "            node_truth = None\n",
    "\n",
    "        loss_weight: Optional[float] = None  # Default\n",
    "        if self._loss_weight_column is not None:\n",
    "            assert self._loss_weight_table is not None\n",
    "            loss_weight_list = self.query_table(\n",
    "                self._loss_weight_table,\n",
    "                self._loss_weight_column,\n",
    "                sequential_index,\n",
    "            )\n",
    "            if len(loss_weight_list):\n",
    "                loss_weight = loss_weight_list[0][0]\n",
    "            else:\n",
    "                loss_weight = -1.0\n",
    "\n",
    "        return features, truth, node_truth, loss_weight\n",
    "\n",
    "    def _create_graph(\n",
    "        self,\n",
    "        features: List[Tuple[float, ...]],\n",
    "        truth: Tuple[Any, ...],\n",
    "        node_truth: Optional[List[Tuple[Any, ...]]] = None,\n",
    "        loss_weight: Optional[float] = None,\n",
    "    ) -> Data:\n",
    "        \"\"\"Create Pytorch Data (i.e. graph) object.\n",
    "\n",
    "        No preprocessing is performed at this stage, just as no node adjancency\n",
    "        is imposed. This means that the `edge_attr` and `edge_weight`\n",
    "        attributes are not set.\n",
    "\n",
    "        Args:\n",
    "            features: List of tuples, containing event features.\n",
    "            truth: List of tuples, containing truth information.\n",
    "            node_truth: List of tuples, containing node-level truth.\n",
    "            loss_weight: A weight associated with the event for weighing the\n",
    "                loss.\n",
    "\n",
    "        Returns:\n",
    "            Graph object.\n",
    "        \"\"\"\n",
    "        # Convert nested list to simple dict\n",
    "        truth_dict = {\n",
    "            key: truth[index] for index, key in enumerate(self._truth)\n",
    "        }\n",
    "\n",
    "        # Define custom labels\n",
    "        labels_dict = self._get_labels(truth_dict)\n",
    "\n",
    "        # Convert nested list to simple dict\n",
    "        if node_truth is not None:\n",
    "            node_truth_array = np.asarray(node_truth)\n",
    "            assert self._node_truth is not None\n",
    "            node_truth_dict = {\n",
    "                key: node_truth_array[:, index]\n",
    "                for index, key in enumerate(self._node_truth)\n",
    "            }\n",
    "\n",
    "        # Catch cases with no reconstructed pulses\n",
    "        if len(features):\n",
    "            data = np.asarray(features)[:, 1:]\n",
    "        else:\n",
    "            data = np.array([]).reshape((0, len(self._features) - 1))\n",
    "\n",
    "        # Construct graph data object\n",
    "        x = torch.tensor(data, dtype=self._dtype)  # pylint: disable=C0103\n",
    "        n_pulses = torch.tensor(len(x), dtype=torch.int32)\n",
    "        graph = Data(x=x, edge_index=None)\n",
    "        graph.n_pulses = n_pulses\n",
    "        graph.features = self._features[1:]\n",
    "\n",
    "        # Add loss weight to graph.\n",
    "        if loss_weight is not None and self._loss_weight_column is not None:\n",
    "            # No loss weight was retrieved, i.e., it is missing for the current\n",
    "            # event.\n",
    "            if loss_weight < 0:\n",
    "                if self._loss_weight_default_value is None:\n",
    "                    raise ValueError(\n",
    "                        \"At least one event is missing an entry in \"\n",
    "                        f\"{self._loss_weight_column} \"\n",
    "                        \"but loss_weight_default_value is None.\"\n",
    "                    )\n",
    "                graph[self._loss_weight_column] = torch.tensor(\n",
    "                    self._loss_weight_default_value, dtype=self._dtype\n",
    "                ).reshape(-1, 1)\n",
    "            else:\n",
    "                graph[self._loss_weight_column] = torch.tensor(\n",
    "                    loss_weight, dtype=self._dtype\n",
    "                ).reshape(-1, 1)\n",
    "\n",
    "        # Write attributes, either target labels, truth info or original\n",
    "        # features.\n",
    "        add_these_to_graph = [labels_dict, truth_dict]\n",
    "        if node_truth is not None:\n",
    "            add_these_to_graph.append(node_truth_dict)\n",
    "        for write_dict in add_these_to_graph:\n",
    "            for key, value in write_dict.items():\n",
    "                try:\n",
    "                    graph[key] = torch.tensor(value)\n",
    "                except TypeError:\n",
    "                    # Cannot convert `value` to Tensor due to its data type,\n",
    "                    # e.g. `str`.\n",
    "                    self.debug(\n",
    "                        (\n",
    "                            f\"Could not assign `{key}` with type \"\n",
    "                            f\"'{type(value).__name__}' as attribute to graph.\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # Additionally add original features as (static) attributes\n",
    "        for index, feature in enumerate(graph.features):\n",
    "            if feature not in [\"x\"]:\n",
    "                graph[feature] = graph.x[:, index].detach()\n",
    "\n",
    "        # Add custom labels to the graph\n",
    "        for key, fn in self._label_fns.items():\n",
    "            graph[key] = fn(graph)\n",
    "        return graph\n",
    "\n",
    "    def _get_labels(self, truth_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Return dictionary of  labels, to be added as graph attributes.\"\"\"\n",
    "        if \"pid\" in truth_dict.keys():\n",
    "            abs_pid = abs(truth_dict[\"pid\"])\n",
    "            sim_type = truth_dict[\"sim_type\"]\n",
    "\n",
    "            labels_dict = {\n",
    "                self._index_column: truth_dict[self._index_column],\n",
    "                \"muon\": int(abs_pid == 13),\n",
    "                \"muon_stopped\": int(truth_dict.get(\"stopped_muon\") == 1),\n",
    "                \"noise\": int((abs_pid == 1) & (sim_type != \"data\")),\n",
    "                \"neutrino\": int(\n",
    "                    (abs_pid != 13) & (abs_pid != 1)\n",
    "                ),  # @TODO: `abs_pid in [12,14,16]`?\n",
    "                \"v_e\": int(abs_pid == 12),\n",
    "                \"v_u\": int(abs_pid == 14),\n",
    "                \"v_t\": int(abs_pid == 16),\n",
    "                \"track\": int(\n",
    "                    (abs_pid == 14) & (truth_dict[\"interaction_type\"] == 1)\n",
    "                ),\n",
    "                \"dbang\": self._get_dbang_label(truth_dict),\n",
    "                \"corsika\": int(abs_pid > 20),\n",
    "            }\n",
    "        else:\n",
    "            labels_dict = {\n",
    "                self._index_column: truth_dict[self._index_column],\n",
    "                \"muon\": -1,\n",
    "                \"muon_stopped\": -1,\n",
    "                \"noise\": -1,\n",
    "                \"neutrino\": -1,\n",
    "                \"v_e\": -1,\n",
    "                \"v_u\": -1,\n",
    "                \"v_t\": -1,\n",
    "                \"track\": -1,\n",
    "                \"dbang\": -1,\n",
    "                \"corsika\": -1,\n",
    "            }\n",
    "        return labels_dict\n",
    "\n",
    "    def _get_dbang_label(self, truth_dict: Dict[str, Any]) -> int:\n",
    "        \"\"\"Get label for double-bang classification.\"\"\"\n",
    "        try:\n",
    "            label = int(truth_dict[\"dbang_decay_length\"] > -1)\n",
    "            return label\n",
    "        except KeyError:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLiteDataset(Dataset):\n",
    "    \"\"\"Pytorch dataset for reading data from SQLite databases.\"\"\"\n",
    "\n",
    "    # Implementing abstract method(s)\n",
    "    def _init(self) -> None:\n",
    "        # Check(s)\n",
    "        self._database_list: Optional[List[str]]\n",
    "        if isinstance(self._path, list):\n",
    "            self._database_list = self._path\n",
    "            self._all_connections_established = False\n",
    "            self._all_connections: List[sqlite3.Connection] = []\n",
    "        else:\n",
    "            self._database_list = None\n",
    "            assert isinstance(self._path, str)\n",
    "            assert self._path.endswith(\n",
    "                \".db\"\n",
    "            ), f\"Format of input file `{self._path}` is not supported.\"\n",
    "\n",
    "        if self._database_list is not None:\n",
    "            self._current_database: Optional[int] = None\n",
    "\n",
    "        # Set custom member variable(s)\n",
    "        self._features_string = \", \".join(self._features)\n",
    "        self._truth_string = \", \".join(self._truth)\n",
    "        if self._node_truth:\n",
    "            self._node_truth_string = \", \".join(self._node_truth)\n",
    "\n",
    "        self._conn: Optional[sqlite3.Connection] = None\n",
    "\n",
    "    def _post_init(self) -> None:\n",
    "        self._close_connection()\n",
    "\n",
    "    def query_table(\n",
    "        self,\n",
    "        table: str,\n",
    "        columns: Union[List[str], str],\n",
    "        sequential_index: Optional[int] = None,\n",
    "        selection: Optional[str] = None,\n",
    "    ) -> List[Tuple[Any, ...]]:\n",
    "        \"\"\"Query table at a specific index, optionally with some selection.\"\"\"\n",
    "        # Check(s)\n",
    "        if isinstance(columns, list):\n",
    "            columns = \", \".join(columns)\n",
    "\n",
    "        if not selection:  # I.e., `None` or `\"\"`\n",
    "            selection = \"1=1\"  # Identically true, to select all\n",
    "\n",
    "        index = self._get_event_index(sequential_index)\n",
    "\n",
    "        # Query table\n",
    "        assert index is not None\n",
    "        self._establish_connection(index)\n",
    "        try:\n",
    "            assert self._conn\n",
    "            if sequential_index is None:\n",
    "                combined_selections = selection\n",
    "            else:\n",
    "                combined_selections = (\n",
    "                    f\"{self._index_column} = {index} and {selection}\"\n",
    "                )\n",
    "\n",
    "            result = self._conn.execute(\n",
    "                f\"SELECT {columns} FROM {table} WHERE \"\n",
    "                f\"{combined_selections}\"\n",
    "            ).fetchall()\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if \"no such column\" in str(e):\n",
    "                raise ColumnMissingException(str(e))\n",
    "            else:\n",
    "                raise e\n",
    "        return result\n",
    "\n",
    "    def _get_all_indices(self) -> List[int]:\n",
    "        self._establish_connection(0)\n",
    "        indices = pd.read_sql_query(\n",
    "            f\"SELECT {self._index_column} FROM {self._truth_table}\", self._conn\n",
    "        )\n",
    "        self._close_connection()\n",
    "        return indices.values.ravel().tolist()\n",
    "\n",
    "    def _get_event_index(\n",
    "        self, sequential_index: Optional[int]\n",
    "    ) -> Optional[int]:\n",
    "        index: int = 0\n",
    "        if sequential_index is not None:\n",
    "            index_ = self._indices[sequential_index]\n",
    "            if self._database_list is None:\n",
    "                assert isinstance(index_, int)\n",
    "                index = index_\n",
    "            else:\n",
    "                assert isinstance(index_, list)\n",
    "                index = index_[0]\n",
    "        return index\n",
    "\n",
    "    # Custom, internal method(s)\n",
    "    # @TODO: Is it necessary to return anything here?\n",
    "    def _establish_connection(self, i: int) -> \"SQLiteDataset\":\n",
    "        \"\"\"Make sure that a sqlite3 connection is open.\"\"\"\n",
    "        if self._database_list is None:\n",
    "            assert isinstance(self._path, str)\n",
    "            if self._conn is None:\n",
    "                self._conn = sqlite3.connect(self._path)\n",
    "        else:\n",
    "            indices = self._indices[i]\n",
    "            assert isinstance(indices, list)\n",
    "            if self._conn is None:\n",
    "                if self._all_connections_established is False:\n",
    "                    self._all_connections = []\n",
    "                    for database in self._database_list:\n",
    "                        con = sqlite3.connect(database)\n",
    "                        self._all_connections.append(con)\n",
    "                    self._all_connections_established = True\n",
    "                self._conn = self._all_connections[indices[1]]\n",
    "            if indices[1] != self._current_database:\n",
    "                self._conn = self._all_connections[indices[1]]\n",
    "                self._current_database = indices[1]\n",
    "        return self\n",
    "\n",
    "    # @TODO: Is it necessary to return anything here?\n",
    "    def _close_connection(self) -> \"SQLiteDataset\":\n",
    "        \"\"\"Make sure that no sqlite3 connection is open.\n",
    "\n",
    "        This is necessary to calls this before passing to\n",
    "        `torch.DataLoader` such that the dataset replica on each worker\n",
    "        is required to create its own connection (thereby avoiding\n",
    "        `sqlite3.DatabaseError: database disk image is malformed` errors\n",
    "        due to inability to use sqlite3 connection accross processes.\n",
    "        \"\"\"\n",
    "        if self._conn is not None:\n",
    "            self._conn.close()\n",
    "            del self._conn\n",
    "            self._conn = None\n",
    "        if self._database_list is not None:\n",
    "            if self._all_connections_established:\n",
    "                for con in self._all_connections:\n",
    "                    con.close()\n",
    "                del self._all_connections\n",
    "                self._all_connections_established = False\n",
    "                self._conn = None\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(graphs: List[Data]) -> Batch:\n",
    "    \"\"\"Remove graphs with less than two DOM hits.\n",
    "\n",
    "    Should not occur in \"production.\n",
    "    \"\"\"\n",
    "    graphs = [g for g in graphs if g.n_pulses > 1]\n",
    "    return Batch.from_data_list(graphs)\n",
    "\n",
    "\n",
    "def do_shuffle(selection_name: str) -> bool:\n",
    "    \"\"\"Check whether to shuffle selection with name `selection_name`.\"\"\"\n",
    "    return \"train\" in selection_name.lower()\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"Class for loading data from a `Dataset`.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        batch_size: int,\n",
    "        shuffle: bool,\n",
    "        num_workers: int = 10,\n",
    "        persistent_workers: bool = True,\n",
    "        collate_fn: Callable = collate_fn,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Construct `DataLoader`.\"\"\"\n",
    "        # Base class constructor\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=2,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_config(\n",
    "        cls,\n",
    "        config: DatasetConfig,\n",
    "        batch_size: int,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[\"DataLoader\", Dict[str, \"DataLoader\"]]:\n",
    "        \"\"\"Construct `DataLoader`s based on selections in `DatasetConfig`.\"\"\"\n",
    "        if isinstance(config.selection, dict):\n",
    "            assert \"shuffle\" not in kwargs, (\n",
    "                \"When passing a `DatasetConfig` with multiple selections, \"\n",
    "                \"`shuffle` is automatically inferred from the selection name, \"\n",
    "                \"and thus should not specified as an argument.\"\n",
    "            )\n",
    "            datasets = Dataset.from_config(config)\n",
    "            assert isinstance(datasets, dict)\n",
    "            data_loaders: Dict[str, DataLoader] = {}\n",
    "            for name, dataset in datasets.items():\n",
    "                data_loaders[name] = cls(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=do_shuffle(name),\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            return data_loaders\n",
    "\n",
    "        else:\n",
    "            assert \"shuffle\" in kwargs, (\n",
    "                \"When passing a `DatasetConfig` with a single selections, you \"\n",
    "                \"need to specify `shuffle` as an argument.\"\n",
    "            )\n",
    "            dataset = Dataset.from_config(config)\n",
    "            assert isinstance(dataset, Dataset)\n",
    "            return cls(dataset, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO: Remove in favour of DataLoader{,.from_dataset_config}\n",
    "def make_dataloader(\n",
    "    db: str,\n",
    "    pulsemaps: Union[str, List[str]],\n",
    "    features: List[str],\n",
    "    truth: List[str],\n",
    "    *,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    selection: Optional[List[int]] = None,\n",
    "    num_workers: int = 10,\n",
    "    persistent_workers: bool = True,\n",
    "    node_truth: List[str] = None,\n",
    "    truth_table: str = \"truth\",\n",
    "    node_truth_table: Optional[str] = None,\n",
    "    string_selection: List[int] = None,\n",
    "    loss_weight_table: Optional[str] = None,\n",
    "    loss_weight_column: Optional[str] = None,\n",
    "    index_column: str = \"event_no\",\n",
    "    labels: Optional[Dict[str, Callable]] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Construct `DataLoader` instance.\"\"\"\n",
    "    # Check(s)\n",
    "    if isinstance(pulsemaps, str):\n",
    "        pulsemaps = [pulsemaps]\n",
    "\n",
    "    dataset = SQLiteDataset(\n",
    "        path=db,\n",
    "        pulsemaps=pulsemaps,\n",
    "        features=features,\n",
    "        truth=truth,\n",
    "        selection=selection,\n",
    "        node_truth=node_truth,\n",
    "        truth_table=truth_table,\n",
    "        node_truth_table=node_truth_table,\n",
    "        string_selection=string_selection,\n",
    "        loss_weight_table=loss_weight_table,\n",
    "        loss_weight_column=loss_weight_column,\n",
    "        index_column=index_column,\n",
    "    )\n",
    "\n",
    "    # adds custom labels to dataset\n",
    "    if isinstance(labels, dict):\n",
    "        for label in labels.keys():\n",
    "            dataset.add_label(key=label, fn=labels[label])\n",
    "\n",
    "    def collate_fn(graphs: List[Data]) -> Batch:\n",
    "        \"\"\"Remove graphs with less than two DOM hits.\n",
    "\n",
    "        Should not occur in \"production.\n",
    "        \"\"\"\n",
    "        graphs = [g for g in graphs if g.n_pulses > 1]\n",
    "        return Batch.from_data_list(graphs)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        persistent_workers=persistent_workers,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# @TODO: Remove in favour of DataLoader{,.from_dataset_config}\n",
    "def make_train_validation_dataloader(\n",
    "    db: str,\n",
    "    selection: Optional[List[int]],\n",
    "    pulsemaps: Union[str, List[str]],\n",
    "    features: List[str],\n",
    "    truth: List[str],\n",
    "    *,\n",
    "    batch_size: int,\n",
    "    database_indices: Optional[List[int]] = None,\n",
    "    seed: int = 42,\n",
    "    test_size: float = 0.33,\n",
    "    num_workers: int = 10,\n",
    "    persistent_workers: bool = True,\n",
    "    node_truth: Optional[str] = None,\n",
    "    truth_table: str = \"truth\",\n",
    "    node_truth_table: Optional[str] = None,\n",
    "    string_selection: Optional[List[int]] = None,\n",
    "    loss_weight_column: Optional[str] = None,\n",
    "    loss_weight_table: Optional[str] = None,\n",
    "    index_column: str = \"event_no\",\n",
    "    labels: Optional[Dict[str, Callable]] = None,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Construct train and test `DataLoader` instances.\"\"\"\n",
    "    # Reproducibility\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "    # Checks(s)\n",
    "    if isinstance(pulsemaps, str):\n",
    "        pulsemaps = [pulsemaps]\n",
    "\n",
    "    if selection is None:\n",
    "        # If no selection is provided, use all events in dataset.\n",
    "        dataset: Dataset\n",
    "        if db.endswith(\".db\"):\n",
    "            dataset = SQLiteDataset(\n",
    "                db,\n",
    "                pulsemaps,\n",
    "                features,\n",
    "                truth,\n",
    "                truth_table=truth_table,\n",
    "                index_column=index_column,\n",
    "            )\n",
    "        elif db.endswith(\".parquet\"):\n",
    "            dataset = ParquetDataset(\n",
    "                db,\n",
    "                pulsemaps,\n",
    "                features,\n",
    "                truth,\n",
    "                truth_table=truth_table,\n",
    "                index_column=index_column,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"File {db} with format {db.split('.'[-1])} not supported.\"\n",
    "            )\n",
    "        selection = dataset._get_all_indices()\n",
    "\n",
    "    # Perform train/validation split\n",
    "    if isinstance(db, list):\n",
    "        df_for_shuffle = pd.DataFrame(\n",
    "            {\"event_no\": selection, \"db\": database_indices}\n",
    "        )\n",
    "        shuffled_df = df_for_shuffle.sample(\n",
    "            frac=1, replace=False, random_state=rng\n",
    "        )\n",
    "        training_df, validation_df = train_test_split(\n",
    "            shuffled_df, test_size=test_size, random_state=rng\n",
    "        )\n",
    "        training_selection = training_df.values.tolist()\n",
    "        validation_selection = validation_df.values.tolist()\n",
    "    else:\n",
    "        training_selection, validation_selection = train_test_split(\n",
    "            selection, test_size=test_size, random_state=rng\n",
    "        )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    common_kwargs = dict(\n",
    "        db=db,\n",
    "        pulsemaps=pulsemaps,\n",
    "        features=features,\n",
    "        truth=truth,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=persistent_workers,\n",
    "        node_truth=node_truth,\n",
    "        truth_table=truth_table,\n",
    "        node_truth_table=node_truth_table,\n",
    "        string_selection=string_selection,\n",
    "        loss_weight_column=loss_weight_column,\n",
    "        loss_weight_table=loss_weight_table,\n",
    "        index_column=index_column,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    training_dataloader = make_dataloader(\n",
    "        shuffle=True,\n",
    "        selection=training_selection,\n",
    "        **common_kwargs,  # type: ignore[arg-type]\n",
    "    )\n",
    "\n",
    "    validation_dataloader = make_dataloader(\n",
    "        shuffle=False,\n",
    "        selection=validation_selection,\n",
    "        **common_kwargs,  # type: ignore[arg-type]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        training_dataloader,\n",
    "        validation_dataloader,\n",
    "    )\n",
    "\n",
    "\n",
    "# @TODO: Remove in favour of Model.predict{,_as_dataframe}\n",
    "def get_predictions(\n",
    "    trainer: Trainer,\n",
    "    model: Model,\n",
    "    dataloader: DataLoader,\n",
    "    prediction_columns: List[str],\n",
    "    *,\n",
    "    node_level: bool = False,\n",
    "    additional_attributes: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get `model` predictions on `dataloader`.\"\"\"\n",
    "    # Gets predictions from model on the events in the dataloader.\n",
    "    # NOTE: dataloader must NOT have shuffle = True!\n",
    "\n",
    "    # Check(s)\n",
    "    if additional_attributes is None:\n",
    "        additional_attributes = []\n",
    "    assert isinstance(additional_attributes, list)\n",
    "\n",
    "    # Set model to inference mode\n",
    "    model.inference()\n",
    "\n",
    "    # Get predictions\n",
    "    predictions_torch = trainer.predict(model, dataloader)\n",
    "    predictions_list = [\n",
    "        p[0].detach().cpu().numpy() for p in predictions_torch\n",
    "    ]  # Assuming single task\n",
    "    predictions = np.concatenate(predictions_list, axis=0)\n",
    "    try:\n",
    "        assert len(prediction_columns) == predictions.shape[1]\n",
    "    except IndexError:\n",
    "        predictions = predictions.reshape((-1, 1))\n",
    "        assert len(prediction_columns) == predictions.shape[1]\n",
    "\n",
    "    # Get additional attributes\n",
    "    attributes: Dict[str, List[np.ndarray]] = OrderedDict(\n",
    "        [(attr, []) for attr in additional_attributes]\n",
    "    )\n",
    "    for batch in dataloader:\n",
    "        for attr in attributes:\n",
    "            attribute = batch[attr].detach().cpu().numpy()\n",
    "            if node_level:\n",
    "                if attr == \"event_no\":\n",
    "                    attribute = np.repeat(\n",
    "                        attribute, batch[\"n_pulses\"].detach().cpu().numpy()\n",
    "                    )\n",
    "            attributes[attr].extend(attribute)\n",
    "\n",
    "    data = np.concatenate(\n",
    "        [predictions]\n",
    "        + [\n",
    "            np.asarray(values)[:, np.newaxis] for values in attributes.values()\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "        data, columns=prediction_columns + additional_attributes\n",
    "    )\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c5fb4c392dc910b689950aeeefba71605df50be4f2015b0a69feb34d143fb9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
