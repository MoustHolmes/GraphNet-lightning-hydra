# @package _global_

# to execute this experiment run:
# python src/train.py experiment=opt_test

defaults:
  - override /data: kaggle_2nd_place_dataset_large_model.yaml
  - override /model: kaggle_2nd_place_model.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["transformer", "kaggle_2nd_place", "energy_log10"]

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 5
  # precision: 16 #16-mixed
  # strategy: ddp
  accelerator: gpu
  devices: [1]
  num_nodes: 1
  # reload_dataloaders_every_n_epochs: 1
  # sync_batchnorm: True
  
model:
  model:
    dim: 768 #192 384 #768
    dim_base: 128
    depth: 12
    use_checkpoint: False
    head_size: 32
    depth_rel: 4
    n_rel: 1
    dim_out: 1


data:
  num_workers: 16
  batch_sizes: [ 2048, 2048, 512,  512, 512, 512, 256, 256, 256, 256, 128, 128, 128, 64, 64, 64, 64, 32, 32, 32, 16, 16, 16, 16 ]


logger:
  wandb:
    tags: ${tags}
    group: "transformer"