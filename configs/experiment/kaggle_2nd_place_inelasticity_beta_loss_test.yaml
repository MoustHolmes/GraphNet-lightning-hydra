# @package _global_

# to execute this experiment run:
# python src/train.py experiment=opt_test

defaults:
  - override /data: kaggle_2nd_place_dataset.yaml
  - override /model: kaggle_2nd_place_model.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["transformer", "kaggle_2nd_place",inelasticity_beta_loss","attention_weights"]

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 16
  # precision: 16 #16-mixed
  # strategy: ddp
  accelerator: gpu
  num_nodes: 1
  # gradient_clip_val: 3.0
  accumulate_grad_batches: 1
  # reload_dataloaders_every_n_epochs: 1
  # sync_batchnorm: True
  # limit_train_batches: 4
  # limit_val_batches: 2
  # limit_test_batches: 20
  
model:
  loss_fn:
    _target_: src.models.components.loss_functions.BetaDistLoss
  model:
    dim: 192 #192 384 #768
    dim_base: 128
    depth: 12
    use_checkpoint: False
    head_size: 32
    depth_rel: 4
    n_rel: 1
    dim_out: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    _partial_: true
    epochs: 16
    steps_per_epoch: 1
    max_lr: 0.0005
    pct_start: 0.01

# callbacks:
#   - beta_prediction_writer_callback.yaml
#   - attention_weight_writer_callback.yaml
  
data:
  target_cols: "inelasticity"
  num_workers: 12


logger:
  wandb:
    tags: ${tags}
    group: "transformer"